{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Naive Bayes\n",
    "\n",
    "## Part 5: Bernoulli naive bayes on real-world data\n",
    "\n",
    "In this notebook you will have to apply the Bernoulli naive bayes algorithm on the dataset of movie reviews.\n",
    "\n",
    "You will have to optimize your code so that this won't take hours to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your name: Benjamin Fraeyman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = load_files('reviews',encoding='us-ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [review.split() for review in reviews.data]\n",
    "X_train = sentences[:150]\n",
    "X_test = sentences[150:]\n",
    "class_vec = reviews.target\n",
    "y_train = class_vec[:150]\n",
    "y_test = class_vec[150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# List which will contain all unique words in the data set\n",
    "all_words = []\n",
    "\n",
    "# Transform the data set (which is a list of lists) to a single list\n",
    "for sentence in X_train:\n",
    "    all_words.extend(sentence)\n",
    "\n",
    "# Use the numpy function \"unique\" to get all unique elements from a list\n",
    "vocab = np.unique(all_words)\n",
    "\n",
    "def encode_binary(vocab,sentence):\n",
    "    vocab_list = vocab.tolist()\n",
    "    binary_sentence=np.zeros(len(vocab_list),)\n",
    "    for word in sentence:\n",
    "        if word in vocab:\n",
    "            binary_sentence[vocab_list.index(word)]=1.\n",
    "    return binary_sentence\n",
    "\n",
    "# apply the function defined above to every sentence in the data set\n",
    "data_set = []\n",
    "for sentence in X_train:\n",
    "    binary_sentence = encode_binary(vocab, sentence)\n",
    "    data_set.append(binary_sentence)\n",
    "    \n",
    "data_set = np.array(data_set)\n",
    "print(data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prior calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior for 0:  0.513333333333\n",
      "Prior for 1:  0.486666666667\n"
     ]
    }
   ],
   "source": [
    "# Calculate the priors\n",
    "# Total number of sentences\n",
    "N = np.float(len(X_train))\n",
    "\n",
    "prior_0 = len(y_train[y_train==0])/N\n",
    "prior_1 = len(y_train[y_train==1])/N\n",
    "\n",
    "print(\"Prior for 0: \", prior_0)\n",
    "print(\"Prior for 1: \", prior_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Likelihood calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_likelihood_0: [0.35443038 0.82278481 0.03797468 ... 0.01265823 0.02531646 0.02531646]\n",
      "words_likelihood_1: [0.26666667 0.70666667 0.01333333 ... 0.02666667 0.01333333 0.01333333]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the likelihood of each word given a class: P(wt|C) \n",
    "# For each word, we want to know in how many documents of a certain class it occured\n",
    "# +1 for the smoothing\n",
    "word_count_class_0 = np.sum(data_set[y_train==0],axis=0) + 1\n",
    "word_count_class_1 = np.sum(data_set[y_train==1],axis=0) + 1\n",
    "\n",
    "# For each class we want to know how many documents belong to it \n",
    "# +2 for laplacian smoothing\n",
    "doc_count_class_0 = len(y_train[y_train==0]) + 2\n",
    "doc_count_class_1 = len(y_train[y_train==1]) + 2\n",
    "\n",
    "# Multiply by 1. to force conversion to floating number\n",
    "words_likelihood_0 = 1. * word_count_class_0 / doc_count_class_0\n",
    "words_likelihood_1 = 1. * word_count_class_1 / doc_count_class_1\n",
    "\n",
    "print(\"words_likelihood_0:\", words_likelihood_0)\n",
    "print(\"words_likelihood_1:\", words_likelihood_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Classification (posterior calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(sentence,vocab,words_likelihood_0,words_likelihood_1,prior_0,prior_1):\n",
    "    coded_sentence = encode_binary(vocab,sentence)\n",
    "    log_likelihood_0 = np.sum((coded_sentence*np.log(words_likelihood_0))+((1-coded_sentence)*np.log(1-words_likelihood_0))) # equation 4 where C=0 \n",
    "    log_likelihood_1 = np.sum((coded_sentence*np.log(words_likelihood_1))+((1-coded_sentence)*np.log(1-words_likelihood_1))) # equation 4 where C=1 \n",
    "    posterior_0 = np.log(prior_0) + log_likelihood_0\n",
    "    posterior_1 = np.log(prior_1) + log_likelihood_1\n",
    "    if posterior_0 > posterior_1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply your classification function on every sentence in the training set (X_train) and store the results in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(0,len(X_train)):\n",
    "    results.append(classify(X_train[i],vocab,words_likelihood_0,words_likelihood_1,prior_0,prior_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know how well the algorithm works, the accuracy score has to be calculated. For more information on the accuracy_score function visit: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def accuracy_score(class_vec,results):\n",
    "    correct = class_vec == results\n",
    "    correct_count = np.count_nonzero(correct == True)\n",
    "    return 1.0*correct_count / len(results);\n",
    "    \n",
    "print(accuracy_score(y_train, results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply your classification function to every sentence in the test set (X_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for i in range(0,len(X_test)):\n",
    "    y_pred.append(classify(X_test[i],vocab,words_likelihood_0,words_likelihood_1,prior_0,prior_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the accuracy score here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.718253968254\n"
     ]
    }
   ],
   "source": [
    "print (accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You also calculated the accuracy on the test set of the movie reviews when multinomial naive bayes was used (exercise 3). Which algorithm performed the best on the test set? Multinomial Naive bayes or Bernoulli Naive bayes?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Multinomial Naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Why did the one perform better than the other? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "BNB: checks for presence of a term\n",
    "\n",
    "MNB: calculates probality a term is present. When a word is not present in the sentence, but is present in the vocabulary, the likelihood calculation for multinomial naive Bayes will not take this into account."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "name": "Lab 5 Bernoulli naive bayes real world example.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
