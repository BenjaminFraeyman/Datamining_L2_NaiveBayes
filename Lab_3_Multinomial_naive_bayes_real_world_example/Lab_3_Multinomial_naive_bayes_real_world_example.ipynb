{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Multinomial Naive Bayes on real-world data\n",
    "\n",
    "The goal of this exercise is to do **sentiment analysis** on movie reviews of imdb. In other words, your machine learning algorithm (multinomial naive Bayes) will have to determine if a movie review is positive or negative.\n",
    "\n",
    "You will have to optimize your code so that this won't take hours to run.\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/69/IMDB_Logo_2016.svg\n",
    "\" alt=\"Drawing\" style=\"height: 150px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your name: Benjamin Fraeyman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and data set creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = load_files('reviews',encoding='us-ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [review.split() for review in reviews.data]\n",
    "X_train = sentences[:150]\n",
    "X_test = sentences[150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_vec = reviews.target\n",
    "y_train = class_vec[:150]\n",
    "y_test = class_vec[150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'!' u'\"' u'$1000' ... u'zweibel' u\"zwick's\" u\"zwigoff's\"]\n"
     ]
    }
   ],
   "source": [
    "#Create the vocabulary\n",
    "# List which will contain all unique words contained in the data set\n",
    "all_words = []\n",
    "\n",
    "# Transform the data set (which is a list of lists) to a single list\n",
    "for sentence in X_train:\n",
    "    all_words.extend(sentence)\n",
    "\n",
    "# Use the numpy function #unique# to get all unique elements from a list\n",
    "vocab = np.unique(all_words)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.  6.  0. ...  0.  0.  0.]\n",
      " [ 0. 24.  0. ...  0.  0.  0.]\n",
      " [ 1.  2.  1. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 7.  8.  0. ...  0.  0.  0.]\n",
      " [ 0.  2.  0. ...  0.  0.  0.]\n",
      " [ 0.  4.  0. ...  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#Encode the training set\n",
    "def encode_multinomial(vocab,sentence):\n",
    "    vocab_list = vocab.tolist()\n",
    "    binary_sentence=np.zeros(len(vocab_list),)\n",
    "    for word in sentence:\n",
    "        if word in vocab:\n",
    "            binary_sentence[vocab_list.index(word)] += 1\n",
    "    return binary_sentence\n",
    "\n",
    "\n",
    "data_set = []\n",
    "for sentence in X_train:\n",
    "    binary_sentence = encode_multinomial(vocab, sentence)\n",
    "    data_set.append(binary_sentence)\n",
    "    \n",
    "data_set = np.array(data_set)\n",
    "\n",
    "print(data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prior calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior for 0:  0.513333333333\n",
      "Prior for 1:  0.486666666667\n"
     ]
    }
   ],
   "source": [
    "# Calculate the priors\n",
    "N = np.float(len(X_train))\n",
    "\n",
    "prior_0 = len(y_train[y_train==0])/N\n",
    "prior_1 = len(y_train[y_train==1])/N\n",
    "\n",
    "print(\"Prior for 0: \", prior_0)\n",
    "print(\"Prior for 1: \", prior_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Likelihood calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_likelihood_0: [1.37448457e-03 9.63701112e-03 4.68574285e-05 ... 1.56191428e-05\n",
      " 3.12382856e-05 3.12382856e-05]\n",
      "words_likelihood_1: [7.33836089e-04 8.27517292e-03 1.56135338e-05 ... 3.12270676e-05\n",
      " 1.56135338e-05 1.56135338e-05]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the P(wt|C) so that it can be used in the next step to calculate the likelihood of a document given a class.\n",
    "# For each word, we want to know in how many documents of a certain class it occured\n",
    "# +1 for the smoothing\n",
    "word_count_class_0 = np.sum(data_set[y_train==0],axis=0) + 1\n",
    "word_count_class_1 = np.sum(data_set[y_train==1],axis=0) + 1\n",
    "\n",
    "# sum of word freq\n",
    "total_count_class_0 = np.sum(data_set[y_train==0])\n",
    "total_count_class_1 = np.sum(data_set[y_train==1])\n",
    "\n",
    "\n",
    "# Multiply by 1. to force conversion to floating number\n",
    "words_likelihood_0 = 1. * word_count_class_0 / (total_count_class_0 + len(vocab))\n",
    "words_likelihood_1 = 1. * word_count_class_1 / (total_count_class_1 + len(vocab))\n",
    "\n",
    "print(\"words_likelihood_0:\", words_likelihood_0)\n",
    "print(\"words_likelihood_1:\", words_likelihood_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification function\n",
    "# Create a function, as in the previous notebook that can classify a new sentence.\n",
    "# The function uses the sentence, the vocabulary, the likelihoods for the two classes and the priors for the two classes.\n",
    "# The function should return the class label for the new sentence\n",
    "def classify(sentence,vocab,words_likelihood_0,words_likelihood_1,prior_0,prior_1):\n",
    "    # Create a BOW representation of the new sentence\n",
    "    coded_sentence = encode_multinomial(vocab,sentence)\n",
    "\n",
    "    # Apply equation (4) to get the likelihoods\n",
    "    log_likelihood_0 = np.sum((coded_sentence*np.log(words_likelihood_0))) # equation 4 where C=0 \n",
    "    log_likelihood_1 = np.sum((coded_sentence*np.log(words_likelihood_1))) # equation 4 where C=1 \n",
    "    \n",
    "    # Apply equation (5) to get the eventual results.\n",
    "    posterior_0 = np.log(prior_0) + log_likelihood_0\n",
    "    posterior_1 = np.log(prior_1) + log_likelihood_1\n",
    "    \n",
    "    # Classify according to equation (6)\n",
    "    if posterior_0 > posterior_1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Apply your classification function on every sentence in the training set (X_train) and store the results in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros(len(X_train))\n",
    "for i in range(0,len(X_train)):\n",
    "    results[i] = classify(X_train[i],vocab,words_likelihood_0,words_likelihood_1,prior_0,prior_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To know how well the algorithm works, the accuracy score has to be calculated. For more information on the accuracy_score function visit: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def accuracy_score(class_vec,results):\n",
    "    correct = class_vec == results\n",
    "    correct_count = np.count_nonzero(correct == True)\n",
    "    return 1.0*correct_count / len(results);\n",
    "    \n",
    "print(accuracy_score(y_train, results))\n",
    "#print(results)\n",
    "#print(zip(np.bincount(results), np.nonzero(results)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply your classification function to every sentence in the test set (X_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.zeros(len(X_test))\n",
    "for i in range(0,len(X_test)):\n",
    "    y_pred[i] = classify(X_test[i],vocab,words_likelihood_0,words_likelihood_1,prior_0,prior_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the accuracy score here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72619047619\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 6. Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which accuracy is the best, the one calculated for the training set or the test set? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is the one better than the other?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Training set is what we used to optimize our algorithm.\n",
    "\n",
    "The test set is harder to get right since it introduces new values into the mix. The best the algorithm can do is makes its guess and it's up to the user of the resulting data to handle it correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Additionally have a look at the confusion matrix: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[96 29]\n",
      " [40 87]]\n",
      "0 are the neg reviews\n",
      "and 1 are the pos reviews\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(96, 29, 40, 87)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "labels = reviews.target_names\n",
    "print(confusion_matrix(y_test,y_pred, labels =[0, 1]))\n",
    "print(\"0 are the %s reviews\" % labels[0])\n",
    "print(\"and 1 are the %s reviews\" % labels[1])\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()\n",
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many positive reviews are classified as positive?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 87"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many positive reviews are classified as negative?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN = 40"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "name": "Lab 3 Multinomial naive bayes real world example.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
