{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Naive Bayes\n",
    "\n",
    "\n",
    "Naive Bayes is a time tested machine learning algorithm that is mostly used for text classification. A famous usage of naive Bayes is for spam filtering.\n",
    "\n",
    "![ETL](https://www.unlocktheinbox.com/images/resource_spam_filters.png)\n",
    "\n",
    "\n",
    "The goal of this lab session is to use two variations of the **naive Bayes** algorithm. Each variation has its specific strengths.\n",
    "\n",
    "The **first** variation is the **Bernoulli naive Bayes** version and is given below. You will have to go through the explanation and code to understand how it works.\n",
    "\n",
    "In the **second** notebook you will have to code **multinomial naive Bayes** yourself.\n",
    "\n",
    "In the **third** notebook you will have to apply the **multinomial naive Bayes** algorithm on a **real-world dataset**. The goal is to automatically determine if a movie review is positive or negative. As this is not a toy dataset, you'll have to **optimize the code** or it will have to run for a long time\n",
    "\n",
    "In the **fourth** notebook you will have to extend the **multinomial naive Bayes** algorithm to work with **more than two classes**.\n",
    "\n",
    "In the **fifth** notebook you will have to apply the **bernouilli naive Bayes** algorithm on **real-world movie reviews dataset** set. Hence, the **code** will have to be **optimized** too.\n",
    "\n",
    "All notebooks should be uploaded before the start of the next lab session.\n",
    "\n",
    "### These exercises should be done individually. This means a submission per student is required. Plagiarism will result in a 0 for the plagiarised parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1: Bernoulli naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bernoulli naive Bayes algorithm works as follows:\n",
    "\n",
    "We want to know what the probability is of a class given a document: $P(C|D)$\n",
    "wherein $D$ is the document and $C$ a class.\n",
    "\n",
    "To calculate this probability Bayes' rule is applied.\n",
    "\n",
    "\\begin{equation*}\n",
    "P(C|D) = \\frac{P(D|C)P(C)}{P(D)} \\propto P(D|C)P(C)\n",
    "\\quad\\quad\\text{(1)}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "- $P(C)$ is called the *prior* and expresses our initial believes of a class occuring in the dataset.\n",
    "- $P(D|C)$ is called the *likelihood* and expresses the probability of observing that document given the class.\n",
    "- The output $P(C|D)$ is called the *posterior*. Hence by applying Bayes' rule we update our initial believes by observing the data.\n",
    "\n",
    "In (1), $\\propto$ means \"proportial to\". We do not calculate P(D) as it is just a scaling factor.\n",
    "\n",
    "Throughout this notebook these different probabilities will be calculated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this notebook 4 steps are done:**\n",
    "\n",
    "1. Data set creation\n",
    "2. Prior calculation\n",
    "3. Likelihood calculation\n",
    "4. Classification (posterior calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data set creation\n",
    "\n",
    "We start of with a small data set:\n",
    "\n",
    "- *class_vec* contains the labels of the samples\n",
    "- *sentences* is a list that contains lists of strings (sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_vec = np.array([0,1,0,1,0,1])\n",
    "sentences = np.array([['my', 'dog', 'has', 'flea', 'problems', 'help', 'please','help'],\n",
    "             ['maybe', 'stop', 'taking', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "             ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "             ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "             ['mr', 'licks', 'ate', 'my', 'steak', 'how','to', 'stop', 'him'],\n",
    "             ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 From text to feature vectors\n",
    "\n",
    "To transform text into feature vector that can be used by the algorithm, the bag of words (BOW) approach is used.\n",
    "\n",
    "**BOW works as follows**:\n",
    "\n",
    "- Get all unique words in the dataset to create a vocabulary ($V$).\n",
    "- For every sentence in the dataset create a binary (Bernoulli) list/array/vector with length equal to the number of elements in the vocabulary ($|V|$).\n",
    "- For every word in a given sentence a *1* is set in the feature vector if it is present in the vocabulary else a *0* is set.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "$V$ = [blue, red, dog, cat, biscuit, apple]\n",
    "\n",
    "The length of $V$ (denoted by $|V|$) is equal to 6.\n",
    "\n",
    "Hence the sentence \"the blue dog ate a blue biscuit\" is encoded as ${\\bf{x}}$=[1,0,1,0,1,0].\n",
    "\n",
    "Note that even though the word *blue* occurs two times, the binary vector solely indicates its presence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Vocabulary creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I' 'ate' 'buying' 'cute' 'dalmation' 'dog' 'flea' 'food' 'garbage' 'has'\n",
      " 'help' 'him' 'how' 'is' 'licks' 'love' 'maybe' 'mr' 'my' 'park' 'please'\n",
      " 'posting' 'problems' 'quit' 'so' 'steak' 'stop' 'stupid' 'taking' 'to'\n",
      " 'worthless']\n"
     ]
    }
   ],
   "source": [
    "# List which will contain all unique words in the data set\n",
    "all_words = []\n",
    "\n",
    "# Transform the data set (which is a list of lists) to a single list\n",
    "for sentence in sentences:\n",
    "    all_words.extend(sentence)\n",
    "\n",
    "# Use the numpy function \"unique\" to get all unique elements from a list\n",
    "vocab = np.unique(all_words)\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Sentences encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_binary(vocab,sentence):\n",
    "    vocab_list = vocab.tolist()\n",
    "    \n",
    "    # Create the binary feature vector and initialize every element as 0\n",
    "    binary_sentence=np.zeros(len(vocab_list),)\n",
    "    \n",
    "    # For every word in the sentence check if it is in the vocabulary,\n",
    "    # if it is: set a 1 on the positions of that element in the binary feature vector\n",
    "    for word in sentence:\n",
    "        if word in vocab:\n",
    "            binary_sentence[vocab_list.index(word)]=1.\n",
    "    return binary_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Data set transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 1. 1. 1. 1. 0.]\n",
      " [1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 1. 1. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      "  0. 1. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 1. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# apply the function defined above to every sentence in the data set\n",
    "data_set = []\n",
    "for sentence in sentences:\n",
    "    binary_sentence = encode_binary(vocab, sentence)\n",
    "    data_set.append(binary_sentence)\n",
    "    \n",
    "data_set = np.array(data_set)\n",
    "print(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first sentence:\n",
      "['my', 'dog', 'has', 'flea', 'problems', 'help', 'please', 'help']\n",
      "\n",
      "A check to see if the sentence was properly encoded\n",
      "['dog' 'flea' 'has' 'help' 'my' 'please' 'problems']\n"
     ]
    }
   ],
   "source": [
    "print(\"The first sentence:\")\n",
    "print(sentences[0])\n",
    "print()\n",
    "print(\"A check to see if the sentence was properly encoded\")\n",
    "print(vocab[data_set[0]==1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prior calculation\n",
    "\n",
    "As prior we calculate the fraction of documents belonging to each class.\n",
    "As there are 6 documents in total and 3 belonging to class 0 (normal) and 3 beloging to class 1 (rude):\n",
    "\n",
    "- $P(C=0)$ = $3/6$\n",
    "- $P(C=1)$ = $3/6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior for 0:  0.5\n",
      "Prior for 1:  0.5\n"
     ]
    }
   ],
   "source": [
    "# Total number of sentences\n",
    "N = np.float(len(sentences))\n",
    "\n",
    "prior_0 = len(class_vec[class_vec==0])/N\n",
    "prior_1 = len(class_vec[class_vec==1])/N\n",
    "\n",
    "print(\"Prior for 0: \", prior_0)\n",
    "print(\"Prior for 1: \", prior_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Likelihood calculation\n",
    "\n",
    "$P(D_i|C)$ is the likelihood of the ith document ($D_i$) in the dataset, given the class $C$.\n",
    "\n",
    "The i-th document is encoded as the feature vector $\\textbf{x}_{i}$.\n",
    "\n",
    "For Bernouilli naive Bayes the likelihood is:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(D_i|C)\\approx P(\\textbf{x}_{i}|C) = \\prod_{t=1}^{|V|}\\Bigl(P(w_t|C)^{x_{it}}(1-P(w_t|C))^{(1-x_{it})}\\Bigr)\n",
    "\\quad\\quad\\text{(2)}\n",
    "\\end{equation*}\n",
    "\n",
    "- $w_t$ is the t-th element (word) in a document\n",
    "- $P(w_t|C)$ is the probability of word $w_t$ occurring in a document of class C\n",
    "- $x_{it}$ is the t-th element in the feature vector $\\textbf{x}_i$\n",
    "\n",
    "This product goes over all words in the vocabulary. If word $w_t$ is present in $D_i$, then $x_{it} = 1$ and the probability of that word occuring in class C is $P(w_t |C)$; if word $w_t$ is not present in $D_i$, then $x_{it} = 0$ and the propability of that word not occuring in C is $1 âˆ’ P(w_t |C)$. **As can be seen, the non-occurance of a word is also taken into account.**\n",
    "\n",
    "To estimate $P(w_t |C)$ we calculate in how many documents, of the given class ($k$), the word $w_t$ is observed (denoted by $n_k(w_t))$. Afterwards, normalization is applied by dividing by the total number of documents belonging to that class (denoted as $N_k$):\n",
    "\n",
    "\\begin{equation*}\n",
    "P(w_t |C=k) = \\frac{1+n_k(w_t)}{2+N_k}\n",
    "\\quad\\quad\\text{(3)}\n",
    "\\end{equation*}\n",
    "\n",
    "Laplace smoothing is also used in (3) so that the likelihood can't be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First $P(w_t |C=k)$  is calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_likelihood_0: [0.4 0.4 0.2 0.4 0.4 0.4 0.4 0.2 0.2 0.4 0.4 0.6 0.4 0.4 0.4 0.4 0.2 0.4\n",
      " 0.8 0.2 0.4 0.2 0.4 0.2 0.4 0.4 0.4 0.2 0.2 0.4 0.2]\n",
      "words_likelihood_1: [0.2 0.2 0.4 0.2 0.2 0.6 0.2 0.4 0.4 0.2 0.2 0.4 0.2 0.2 0.2 0.2 0.4 0.2\n",
      " 0.2 0.4 0.2 0.4 0.2 0.4 0.2 0.2 0.6 0.8 0.4 0.4 0.6]\n"
     ]
    }
   ],
   "source": [
    "# For each word, we want to know in how many documents of a certain class it occured\n",
    "# +1 for the smoothing\n",
    "word_count_class_0 = np.sum(data_set[class_vec==0],axis=0) + 1\n",
    "word_count_class_1 = np.sum(data_set[class_vec==1],axis=0) + 1\n",
    "\n",
    "# For each class we want to know how many documents belong to it \n",
    "# +2 for laplacian smoothing\n",
    "doc_count_class_0 = len(class_vec[class_vec==0]) + 2\n",
    "doc_count_class_1 = len(class_vec[class_vec==1]) + 2\n",
    "\n",
    "# Multiply by 1. to force conversion to floating number\n",
    "words_likelihood_0 = 1. * word_count_class_0 / doc_count_class_0\n",
    "words_likelihood_1 = 1. * word_count_class_1 / doc_count_class_1\n",
    "\n",
    "print(\"words_likelihood_0:\", words_likelihood_0)\n",
    "print(\"words_likelihood_1:\", words_likelihood_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Classification (posterior calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to classify a (new) sentence we need the priors (P(C=0) and P(C=1)) and the likelihoods (P(D|C=0) and P(D|C=1)). The likelihood will be calculated next as described in equation 2.\n",
    "\n",
    "As a probability can be a small number, and multiplying small numbers can lead to precision problems, we can use following rule:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\log(uv) = \\log(u) + \\log(v)\n",
    "\\end{equation*}\n",
    "\n",
    "We apply this rule on equation *(2)*, resulting in:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "P(D_i|C)\\approx P(\\textbf{x}_{i}|C) & = \\prod_{t=1}^{|V|}\\Bigl(P(w_t|C)^{x_{it}}(1-P(w_t|C))^{(1-x_{it})}\\Bigr) \\\\\n",
    "\\log(P(D_i|C))\\approx \\log(P(\\textbf{x}_{i}|C)) & = \\sum_{t=1}^{|V|}\\Bigl(x_{it}\\log(P(w_t|C))+(1-x_{it})\\log(1-P(w_t|C))\\Bigr) \\quad\\quad\\text{(4)}\n",
    "\\end{align}\n",
    "\n",
    " and subsequently on Bayes' rule itself to calculate the posterior:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\log(P(C|D)) \\propto \\log(P(D|C))+\\log(P(C))\n",
    "\\quad\\quad\\text{(5)}\n",
    "\\end{equation*}\n",
    "\n",
    "Using the posterior, the sentence can then be classified as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "      0, & \\text{if}\\ \\log(P(C=0|D_i))> \\log(P(C=1|D_i)) \\\\\n",
    "      1, & \\text{otherwise}\n",
    "\\end{cases}\\quad\\text{(6)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(sentence,vocab,words_likelihood_0,words_likelihood_1,prior_0,prior_1):\n",
    "\n",
    "    # Create a BOW representation of the new sentence\n",
    "    coded_sentence = encode_binary(vocab,sentence)\n",
    "\n",
    "    # Apply equation (4) to get the likelihoods\n",
    "    log_likelihood_0 = np.sum((coded_sentence*np.log(words_likelihood_0))+((1-coded_sentence)*np.log(1-words_likelihood_0))) # equation 4 where C=0 \n",
    "    log_likelihood_1 = np.sum((coded_sentence*np.log(words_likelihood_1))+((1-coded_sentence)*np.log(1-words_likelihood_1))) # equation 4 where C=1 \n",
    "    \n",
    "    # Apply equation (5) to get the eventual results.\n",
    "    posterior_0 = np.log(prior_0) + log_likelihood_0\n",
    "    posterior_1 = np.log(prior_1) + log_likelihood_1\n",
    "    \n",
    "    # Classify according to equation (6)\n",
    "    if posterior_0 > posterior_1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The two sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(['my','dog','is','cute','he','licks','me'],vocab,words_likelihood_0,words_likelihood_1,prior_0,prior_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(['my','dog','is','stupid','and','worthless',\"real\"],vocab,words_likelihood_0,words_likelihood_1,prior_0,prior_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "name": "Lab 1 Bernoulli naive bayes.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
