{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Multinomial naive Bayes for multi-class classification\n",
    "\n",
    "Extend the multinomial naive Bayes solution so that you can distinguish between more than 2 classes. \n",
    "\n",
    "We'll work on a toy dataset again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Benjamin Fraeyman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and data set creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_vec = np.array([0,1,0,1,0,1,2,2,2])\n",
    "sentences = np.array([['my', 'dog', 'has', 'flea', 'problems', 'help', 'please','help'],\n",
    "             ['maybe', 'stop', 'taking', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "             ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "             ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "             ['mr', 'licks', 'ate', 'my', 'steak', 'how','to', 'stop', 'him'],\n",
    "             ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid'],\n",
    "             ['Kitten','is','so','fluffy','white','she','has','little','paws'],\n",
    "             ['Cat','has','long','nails','on','his','paws'],\n",
    "             ['Kitten','is','so','little','she','has','orange','fluffy','fur']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "# List which will contain all unique words contained in the data set\n",
    "all_words = []\n",
    "\n",
    "# Transform the data set (which is a list of lists) to a single list\n",
    "for sentence in sentences:\n",
    "    all_words.extend(sentence)\n",
    "\n",
    "# Use the numpy function #unique# to get all unique elements from a list\n",
    "vocab = np.unique(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0.\n",
      "  1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Encode the data set\n",
    "def encode_multinomial(vocab,sentence):\n",
    "    vocab_list = vocab.tolist()\n",
    "    binary_sentence=np.zeros(len(vocab_list),)\n",
    "    for word in sentence:\n",
    "        if word in vocab:\n",
    "            binary_sentence[vocab_list.index(word)] += 1\n",
    "    return binary_sentence\n",
    "\n",
    "data_set = []\n",
    "for sentence in sentences:\n",
    "    binary_sentence = encode_multinomial(vocab, sentence)\n",
    "    data_set.append(binary_sentence)\n",
    "    \n",
    "data_set = np.array(data_set)\n",
    "print(data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prior calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[0.33333333 0.33333333 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the priors\n",
    "# Total number of sentences\n",
    "N = np.float(len(sentences))\n",
    "class_unique = np.unique(class_vec)\n",
    "print(len(class_unique))\n",
    "prior = np.zeros(len(class_unique))\n",
    "for sentence_class in (class_unique):\n",
    "    prior[sentence_class] = len(class_vec[class_vec==sentence_class])/N\n",
    "\n",
    "print(prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Likelihood calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[array([0.01449275, 0.02898551, 0.01449275, 0.02898551, 0.01449275,\n",
      "       0.02898551, 0.02898551, 0.02898551, 0.02898551, 0.01449275,\n",
      "       0.01449275, 0.01449275, 0.01449275, 0.02898551, 0.04347826,\n",
      "       0.04347826, 0.01449275, 0.02898551, 0.02898551, 0.02898551,\n",
      "       0.01449275, 0.01449275, 0.02898551, 0.01449275, 0.02898551,\n",
      "       0.05797101, 0.01449275, 0.01449275, 0.01449275, 0.01449275,\n",
      "       0.01449275, 0.02898551, 0.01449275, 0.02898551, 0.01449275,\n",
      "       0.01449275, 0.02898551, 0.02898551, 0.02898551, 0.01449275,\n",
      "       0.01449275, 0.02898551, 0.01449275, 0.01449275]), array([0.01587302, 0.01587302, 0.01587302, 0.01587302, 0.03174603,\n",
      "       0.01587302, 0.01587302, 0.04761905, 0.01587302, 0.01587302,\n",
      "       0.03174603, 0.01587302, 0.03174603, 0.01587302, 0.01587302,\n",
      "       0.03174603, 0.01587302, 0.01587302, 0.01587302, 0.01587302,\n",
      "       0.01587302, 0.01587302, 0.01587302, 0.03174603, 0.01587302,\n",
      "       0.01587302, 0.01587302, 0.01587302, 0.01587302, 0.03174603,\n",
      "       0.01587302, 0.01587302, 0.03174603, 0.01587302, 0.03174603,\n",
      "       0.01587302, 0.01587302, 0.01587302, 0.04761905, 0.06349206,\n",
      "       0.03174603, 0.03174603, 0.01587302, 0.04761905]), array([0.02898551, 0.01449275, 0.04347826, 0.01449275, 0.01449275,\n",
      "       0.01449275, 0.01449275, 0.01449275, 0.01449275, 0.04347826,\n",
      "       0.01449275, 0.02898551, 0.01449275, 0.05797101, 0.01449275,\n",
      "       0.01449275, 0.02898551, 0.01449275, 0.04347826, 0.01449275,\n",
      "       0.04347826, 0.02898551, 0.01449275, 0.01449275, 0.01449275,\n",
      "       0.01449275, 0.02898551, 0.02898551, 0.02898551, 0.01449275,\n",
      "       0.04347826, 0.01449275, 0.01449275, 0.01449275, 0.01449275,\n",
      "       0.04347826, 0.04347826, 0.01449275, 0.01449275, 0.01449275,\n",
      "       0.01449275, 0.01449275, 0.02898551, 0.01449275])]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the likelihood of each word given a class: P(wt|C) \n",
    "class_count = len(np.unique(class_vec))\n",
    "word_count_class = []\n",
    "total_word_count_class = []\n",
    "words_likelihood = []\n",
    "\n",
    "print(class_unique)\n",
    "for sentence_class in range(0, class_count):\n",
    "    word_count_class.append(np.sum(data_set[class_vec==sentence_class],axis=0) + 1)\n",
    "    total_word_count_class.append(np.sum(data_set[class_vec==sentence_class]))\n",
    "    words_likelihood.append(1. * word_count_class[sentence_class] / (total_word_count_class[sentence_class] + len(vocab)))\n",
    "\n",
    "print(words_likelihood)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification function that returns the label\n",
    "def classify(sentence,vocab,words_likelihood,prior):\n",
    "    # Create a BOW representation of the new sentence\n",
    "    coded_sentence = encode_multinomial(vocab,sentence)\n",
    "    class_count = len(words_likelihood)\n",
    "    log_likelihood = []\n",
    "    posterior = []\n",
    "    \n",
    "    for sentence_class in range(0, class_count):\n",
    "        log_likelihood.append(np.sum((coded_sentence*np.log(words_likelihood[sentence_class]))))\n",
    "        posterior.append(np.log(prior[sentence_class])+log_likelihood[sentence_class])\n",
    "\n",
    "    highest_value = -float(\"inf\")\n",
    "    highest_class = 0\n",
    "    for sentence_class in range(0, class_count):\n",
    "        if posterior[sentence_class] > highest_value:\n",
    "            highest_value = posterior[sentence_class]\n",
    "            highest_class = sentence_class\n",
    "    return highest_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classify this sentence and print the label\n",
    "sentence1=['my','dog','is','cute','he','licks','me']\n",
    "classify(sentence1, vocab, words_likelihood, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classify this sentence and print the label\n",
    "sentence2=['my','dog','is','stupid','and','worthless',\"real\"]\n",
    "classify(sentence2, vocab, words_likelihood, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classify this sentence and print the label\n",
    "sentence3=['she','is','so','white','and','fluffy',\"my\",\"little\",\"kitten\"]\n",
    "classify(sentence3, vocab, words_likelihood, prior)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "name": "Lab 4 Multinomial naive bayes multi class.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
