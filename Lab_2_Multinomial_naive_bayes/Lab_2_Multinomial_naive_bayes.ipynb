{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Multinomial naive Bayes\n",
    "\n",
    "As with Bernoulli naive Bayes, Bayes' rule is applied to calculate the propability of a class given the data. However, for multinomial naive Bayes, sentences are not encoded as binary vector, but vectors containing the word count.\n",
    "\n",
    "Suppose that $\\textbf{x}_{i}$ is the feature vector describing a document, then $x_{it}$ will contain how many times the t-th term occurs in the i-th document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Benjamin Fraeyman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and data set creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_vec = np.array([0,1,0,1,0,1])\n",
    "sentences = np.array([['my', 'dog', 'has', 'flea', 'problems', 'help', 'please','help'],\n",
    "             ['maybe', 'stop', 'taking', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "             ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "             ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "             ['mr', 'licks', 'ate', 'my', 'steak', 'how','to', 'stop', 'him'],\n",
    "             ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List which will contain all unique words contained in the data set\n",
    "all_words = []\n",
    "\n",
    "# Transform the data set (which is a list of lists) to a single list\n",
    "for sentence in sentences:\n",
    "    all_words.extend(sentence)\n",
    "\n",
    "# Use the numpy function #unique# to get all unique elements from a list\n",
    "vocab = np.unique(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Instead of indicating if a term of the vocabulary is present in a sentence, count how many times it present in the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_multinomial(vocab,sentence):\n",
    "    vocab_list = vocab.tolist()\n",
    "    binary_sentence=np.zeros(len(vocab_list),)\n",
    "    for word in sentence:\n",
    "        if word in vocab:\n",
    "            binary_sentence[vocab_list.index(word)] += 1\n",
    "    return binary_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 1. 1. 1. 1. 0.]\n",
      " [1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 1. 1. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      "  0. 1. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 1. 0. 0. 1.]]\n",
      "The first sentence:\n",
      "['my', 'dog', 'has', 'flea', 'problems', 'help', 'please', 'help']\n",
      "A check to see if the sentence was properly encoded\n",
      "['dog' 'flea' 'has' 'help' 'my' 'please' 'problems']\n"
     ]
    }
   ],
   "source": [
    "# apply the function defined above to every sentence in the data set to create a new data set containing the tokenized sentences\n",
    "data_set = []\n",
    "for sentence in sentences:\n",
    "    binary_sentence = encode_multinomial(vocab, sentence)\n",
    "    data_set.append(binary_sentence)\n",
    "    \n",
    "data_set = np.array(data_set)\n",
    "print(data_set)\n",
    "\n",
    "print(\"The first sentence:\")\n",
    "print(sentences[0])\n",
    "print(\"A check to see if the sentence was properly encoded\")\n",
    "print(vocab[data_set[0]!=0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prior calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior for 0:  0.5\n",
      "Prior for 1:  0.5\n"
     ]
    }
   ],
   "source": [
    "#calculate the priors\n",
    "# Total number of sentences\n",
    "N = np.float(len(sentences))\n",
    "\n",
    "prior_0 = len(class_vec[class_vec==0])/N\n",
    "prior_1 = len(class_vec[class_vec==1])/N\n",
    "\n",
    "print(\"Prior for 0: \", prior_0)\n",
    "print(\"Prior for 1: \", prior_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Likelihood calculation\n",
    "\n",
    "The likelihood for multinomial naive Bayes is calculated as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(D_i|C)\\approx P(\\textbf{x}_i|C) = \\prod_{t=1}^{|V|} P(w_t|C)^{N_{it}}\n",
    "\\quad\\quad\\text{(1)}\n",
    "\\end{equation*}\n",
    "\n",
    "- $N_{it}$: the number of time word $w_t$ occurs in $D_i$\n",
    "\n",
    "$P(w_t|C=k)$ can be determined by calculating how many times word $w_t$ occurs in documents belonging to class $k$ divided by the total number of words that are present in documents belonging to class $k$.\n",
    "\n",
    "To apply smoothing, add 1 to the nominator, and $|V|$ to the denominator.\n",
    "\n",
    "Formally:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(w_t|C=k) = \\frac{1 + \\text{tf(}w_t,D \\in k\\text{)}}{|V| + \\sum_i \\text{tf(}w_i,D \\in k\\text{)}}\n",
    "\\quad\\quad\\text{(2)}\n",
    "\\end{equation*}\n",
    "\n",
    "- tf($w_t$,$D \\in k$): term frequency, i.e. how many times $w_t$ occurs in documents of class $k$\n",
    "- $|V|$: the size of your vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_likelihood_0: [0.03571429 0.03571429 0.01785714 0.03571429 0.03571429 0.03571429\n",
      " 0.03571429 0.01785714 0.01785714 0.03571429 0.05357143 0.05357143\n",
      " 0.03571429 0.03571429 0.03571429 0.03571429 0.01785714 0.03571429\n",
      " 0.07142857 0.01785714 0.03571429 0.01785714 0.03571429 0.01785714\n",
      " 0.03571429 0.03571429 0.03571429 0.01785714 0.01785714 0.03571429\n",
      " 0.01785714]\n",
      "words_likelihood_1: [0.02 0.02 0.04 0.02 0.02 0.06 0.02 0.04 0.04 0.02 0.02 0.04 0.02 0.02\n",
      " 0.02 0.02 0.04 0.02 0.02 0.04 0.02 0.04 0.02 0.04 0.02 0.02 0.06 0.08\n",
      " 0.04 0.04 0.06]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the P(wt|C) so that it can be used in the next step to calculate the likelihood of a document given a class.\n",
    "# For each word, we want to know in how many documents of a certain class it occured\n",
    "# +1 for the smoothing\n",
    "word_count_class_0 = np.sum(data_set[class_vec==0],axis=0) + 1\n",
    "word_count_class_1 = np.sum(data_set[class_vec==1],axis=0) + 1\n",
    "\n",
    "# sum of word freq\n",
    "total_count_class_0 = np.sum(data_set[class_vec==0])\n",
    "total_count_class_1 = np.sum(data_set[class_vec==1])\n",
    "\n",
    "\n",
    "# Multiply by 1. to force conversion to floating number\n",
    "words_likelihood_0 = 1. * word_count_class_0 / (total_count_class_0 + len(vocab))\n",
    "words_likelihood_1 = 1. * word_count_class_1 / (total_count_class_1 + len(vocab))\n",
    "\n",
    "print(\"words_likelihood_0:\", words_likelihood_0)\n",
    "print(\"words_likelihood_1:\", words_likelihood_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification (posterior calculation)\n",
    "\n",
    "To classify a (new) sentence we need the priors (P(C=0) and P(C=1)) and the likelihoods (P(D|C=0) and P(D|C=1)). The likelihood will be calculated next using P(wt|C).\n",
    "\n",
    "As a probability can be a small number, and multiplying small numbers can lead to precision problems, the following logarithmic rule is applied:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\log(uv) = \\log(u) + \\log(v)\n",
    "\\end{equation*}\n",
    "\n",
    "We apply this rule on equation *(1)*, resulting in:\n",
    "\n",
    "\\begin{align}\n",
    "P(D_i|C)\\approx P(\\textbf{x}_i|C) & = \\prod_{t=1}^{|V|} P(w_t|C)^{N_{it}} \\\\\n",
    "\\log(P(D_i|C)) \\approx log(P(\\textbf{x}_i|C)) & = \\sum_{t=1}^{|V|} N_{it} \\log(P(w_t|C)) \\quad\\quad\\text{(3)}\n",
    "\\end{align}\n",
    "\n",
    " and subsequently on Bayes' rule itself to calculate the posterior:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\log(P(C|D)) \\propto \\log(P(D|C))+\\log(P(C))\n",
    "\\quad\\quad\\text{(4)}\n",
    "\\end{equation*}\n",
    "\n",
    "Note here, that when a word is not present in the sentence, but is present in the vocabulary, the likelihood calculation for multinomial naive Bayes will not take this into account.\n",
    "\n",
    "Eventually, the sentence can then be classified as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "      0, & \\text{if}\\ \\log(P(C=0|D_i))> \\log(P(C=1|D_i)) \\\\\n",
    "      1, & \\text{otherwise}\n",
    "\\end{cases}\\quad\\text{(5)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function, as in the previous notebook that can classify a new sentence.\n",
    "# The function uses the sentence, the vocabulary, the likelihoods for the two classes and the priors for the two classes.\n",
    "# The function should return the class label for the new sentence\n",
    "def classify(sentence,vocab,words_likelihood_0,words_likelihood_1,prior_0,prior_1):\n",
    "    # Create a BOW representation of the new sentence\n",
    "    coded_sentence = encode_multinomial(vocab,sentence)\n",
    "\n",
    "    # Apply equation (4) to get the likelihoods\n",
    "    log_likelihood_0 = np.sum((coded_sentence*np.log(words_likelihood_0))) # equation 4 where C=0 \n",
    "    log_likelihood_1 = np.sum((coded_sentence*np.log(words_likelihood_1))) # equation 4 where C=1 \n",
    "    \n",
    "    # Apply equation (5) to get the eventual results.\n",
    "    posterior_0 = np.log(prior_0) + log_likelihood_0\n",
    "    posterior_1 = np.log(prior_1) + log_likelihood_1\n",
    "    \n",
    "    # Classify according to equation (6)\n",
    "    if posterior_0 > posterior_1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to classify the two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = ['my','dog','is','cute','he','licks','me']\n",
    "# Use your classify function\n",
    "classify(sentence1,vocab,words_likelihood_0,words_likelihood_1,prior_0,prior_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2 = ['my','dog','is','stupid','and','worthless',\"real\"]\n",
    "classify(sentence2,vocab,words_likelihood_0,words_likelihood_1,prior_0,prior_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "name": "Lab 2 Multinomial naive bayes.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
